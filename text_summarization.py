# -*- coding: utf-8 -*-
"""Text_summarization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Tr-fA6c8_vEuThMS01MYzL4KBUQbYodU
"""

!pip install wikipedia

import wikipedia
wikipage=wikipedia.page('Artificial_intelligence').content

from IPython.display import HTML,display
def set_css():
  display(HTML('''
  <style>
    pre{
      white-space: pre-wrap;

    }
  </style>
  '''))
get_ipython().events.register('pre_run_cell',set_css)

print(wikipage)

import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
wiki_sent=[]
wiki_sent=sent_tokenize(wikipage)
print(wiki_sent)

from nltk.tokenize import word_tokenize
wiki_words=[]
for sent in wiki_sent:
  wiki_words.extend(word_tokenize(sent))
print(wiki_words)

from nltk.corpus import stopwords
nltk.download('stopwords')
woed_freq={}

stopwords=stopwords.words('english')
for word in wiki_words:
  if word not in stopwords and word.isalpha():
    if word not in woed_freq.keys():
      woed_freq[word]=1
    else:
      woed_freq[word]+=1
print(woed_freq)

max_freq=max(woed_freq.values())
for word in woed_freq.keys():
  woed_freq[word]=(woed_freq[word]/max_freq)
print(woed_freq)

sentence_scores={}
for sent in wiki_sent:
  for word in wiki_words:
    if word.lower() in woed_freq.keys():
      if len(sent.split(' ')) < 30:

        if sent not in sentence_scores.keys():
          sentence_scores[sent]=woed_freq[word]
        else:
          sentence_scores[sent]+=woed_freq[word]
print(sentence_scores)

import heapq
summary_sent=heapq.nlargest(7,sentence_scores,key=sentence_scores.get)

summary=' '.join(summary_sent)
print(summary)



















































